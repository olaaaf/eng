\chapter[Przegląd literatury]{PrzeglĄd literatury}
\label{chap:przeglad}
Uczenie ze wzmocnieniem (ang. Reinforcement Learning, RL) jest jednym z najważniejszych podejść do sztucznej inteligencji, szczególnie w kontekście sterowania agentami w złożonych środowiskach, takich jak gry wideo. W literaturze naukowej można znaleźć liczne prace badające różne aspekty i ulepszenia algorytmów RL, które w znaczący sposób przyczyniły się do rozwoju tej dziedziny. W szczególności, prace poświęcone grze Super Mario Bros oraz innym platformom do gier, takim jak Atari czy Nintendo Entertainment System (NES), pozwalają lepiej zrozumieć wyzwania stojące przed algorytmami RL.

W pracy LeBlanca i Lee \cite{NES} autorzy badali zastosowanie uczenia ze wzmocnieniem wśrodowisku konsoli do gier NES. Autorzy wykorzystali agentów RL, którzy byli trenowani bez dostarczania wiedzy eksperckiej, co pozwoliło na zbadanie, jak dobrze nowoczesne algorytmy są w stanie sobie radzić w środowiskach o dużym stopniu złożoności. W pracy omówiono zmiany w hiperparametrach i funkcjach nagród, które są niezbędne do skutecznego rozwiązania problemów w grach NES, wykorzystując dane pikselowe jako jedyne wejście do sieci neuronowych agentów.

Hessel i współpracownicy \cite{RAI} przeanalizowali szereg niezależnych ulepszeń algorytmu DQN (Deep Q-Network), które wprowadzono w ostatnich latach w społeczności zajmującej się uczeniem ze wzmocnieniem. W pracy połączono sześć kluczowych rozszerzeń DQN, które wcześniej były badane oddzielnie, aby ocenić ich komplementarność. Wyniki eksperymentów przeprowadzonych na konsoli Atari 2600 wykazały, że połączenie tych ulepszeń skutkuje znacznym zwiększeniem efektywności algorytmu zarówno pod względem wydajności danych, jak i ostatecznej jakości wyników.

Salimans i inni \cite{EV} przedstawili alternatywne podejście do metod opartych na procesach decyzyjnych Markowa (MDP), takich jak Q-learning i Policy Gradients, poprzez zastosowanie strategii ewolucyjnych (ES). ES, jako technika optymalizacji „czarnej skrzynki”, charakteryzuje się dużą skalowalnością w zależności od liczby dostępnych jednostek obliczeniowych (CPU). W pracy wykazano, że zastosowanie tej metody umożliwia równoległe trenowanie agentów na dużą skalę, co pozwala na szybkie rozwiązywanie złożonych problemów, takich jak poruszanie się humanoida w symulatorze 3D w 10 minut oraz osiągnięcie konkurencyjnych wyników w większości gier Atari po godzinie treningu. ES wykazuje również tolerancję na długie horyzonty czasowe oraz niewrażliwość na częstotliwość akcji i opóźnione nagrody, co jest istotne w grach o dużej złożoności, takich jak Super Mario Bros.

Van Hasselt i współautorzy \cite{DQ} skoncentrowali się na problemie przeszacowania wartości akcji w popularnym algorytmie Q-learning, który może prowadzić do gorszej wydajności w pewnych warunkach. Autorzy przedstawili algorytm Double Q-learning, który redukuje ten problem, oddzielając wybór akcji od jej oceny. Zastosowanie tej techniki w algorytmie DQN pozwoliło na znaczne zmniejszenie przeszacowania wartości w grach Atari, co z kolei prowadziło do lepszych wyników w porównaniu do standardowego DQN.

Na koniec, książka Gérona \cite{HML} dostarcza kompleksowego omówienia technik uczenia maszynowego, w tym uczenia ze wzmocnieniem, z naciskiem na narzędzia takie jak Scikit-Learn i TensorFlow. Książka ta stanowi doskonałe źródło wiedzy dla inżynierów i badaczy, którzy chcą wdrożyć nowoczesne techniki uczenia maszynowego, w tym RL, do rzeczywistych systemów, takich jak gry wideo.

