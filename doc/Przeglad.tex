\setcounter{secnumdepth}{3}
%\RedeclareSectionCommand[runin=false,afterskip=0pt,afterindent=false]{paragraph}
\chapter[Przegląd literatury]{PrzeglĄd literatury}
\label{chap:przeglad}
Uczenie ze wzmocnieniem (ang. Reinforcement Learning, RL) jest jednym z najważniejszych podejść do sztucznej inteligencji, szczególnie w kontekście sterowania agentami w złożonych środowiskach, takich jak gry wideo. W literaturze naukowej można znaleźć liczne prace badające różne aspekty i ulepszenia algorytmów RL, które w znaczący sposób przyczyniły się do rozwoju tej dziedziny. W szczególności, prace poświęcone grze Super Mario Bros oraz innym platformom do gier, takim jak Atari czy Nintendo Entertainment System (NES), pozwalają lepiej zrozumieć wyzwania stojące przed algorytmami RL.

\section{Rainbow: Integracja usprawnień w głębokim uczeniu ze wzmocnieniem}

Algorytmy uczenia ze wzmocnieniem (Reinforcement Learning, RL) ewoluowały przez lata, zyskując na popularności dzięki sukcesom takich algorytmów jak Deep Q-Network (DQN), który pozwolił na uczenie agentów bezpośrednio z surowych danych, takich jak obrazy, bez potrzeby projektowania cech wejściowych. Pomimo znaczących postępów, algorytm DQN posiadał pewne wady, które ograniczały jego efektywność. Aby przezwyciężyć te ograniczenia, badacze opracowali wiele usprawnień, które miały na celu poprawienie stabilności, wydajności i zdolności uczenia się w bardziej złożonych środowiskach. W pracy Hessela i in. \cite{RAI} autorzy połączyli kilka kluczowych ulepszeń DQN w ramach jednego zintegrowanego algorytmu, który nazwano "Rainbow".

Rainbow integruje sześć głównych rozszerzeń DQN: Double Q-learning, priorytetyzowaną pamięć doświadczeń, architekturę sieci dueling, uczenie wieloetapowe, uczenie dystrybucyjne oraz sieci z warstwami szumu (Noisy Nets). Celem było sprawdzenie, które z tych usprawnień można łączyć w sposób komplementarny i jak takie połączenie wpływa na ostateczną wydajność algorytmu.

\subsection{Double Q-learning}
Double Q-learning, zaproponowany przez van Hasselta \cite{RAI}, był jednym z pierwszych rozszerzeń wprowadzonych do DQN w celu ograniczenia problemu przeszacowania wartości akcji. W standardowym Q-learningu maksymalizacja wartości akcji wprowadza tendencję do preferowania zawyżonych wartości, co może prowadzić do niestabilności w procesie uczenia. W Double Q-learningu wybór akcji jest oddzielony od jej oceny, co zmniejsza ten problem. W Rainbow zastosowano tę technikę, aby poprawić stabilność algorytmu i zmniejszyć ryzyko nadmiernego optymizmu, co prowadziło do lepszych wyników w grach Atari.

\subsection{Priorytetyzowana pamięć doświadczeń}
Priorytetyzowana pamięć doświadczeń (ang. Prioritized Experience Replay), wprowadzona przez Schaula i in., umożliwia częstsze wykorzystywanie doświadczeń, które mają większą wartość dla procesu uczenia. Zamiast losowego wybierania próbek do uczenia, algorytm priorytetyzuje te, które charakteryzują się wyższą różnicą błędu (TD-error), co oznacza, że agent może szybciej uczyć się na przykładach, które dostarczają więcej informacji. W kontekście Rainbow, ten mechanizm znacząco poprawił wydajność algorytmu, pozwalając agentom na szybsze uczenie się w złożonych środowiskach.

\subsection{Architektura sieci dueling}
Kolejnym kluczowym usprawnieniem w Rainbow jest wykorzystanie architektury sieci dueling (ang. Dueling Network Architecture), zaproponowanej przez Wanga i in. \cite{RAI}. W tradycyjnych sieciach neuronowych dla RL każda akcja jest bezpośrednio przypisana do konkretnej wartości. Jednak architektura dueling rozdziela ocenę wartości stanu (state value) od oceny przewagi danej akcji (advantage of action). Dzięki temu sieć jest w stanie lepiej oceniać sytuacje, w których różnice między poszczególnymi akcjami są małe, co przyczynia się do bardziej efektywnej generalizacji i lepszych wyników.

\subsection{Uczenie wieloetapowe}
Uczenie wieloetapowe (ang. Multi-step Learning) przesuwa kompromis pomiędzy błędem a wariancją, co umożliwia szybsze propagowanie nagród do wcześniejszych stanów. W tradycyjnym Q-learningu agent uczy się na podstawie jednej nagrody i używa zachłannej akcji w następnym kroku do bootstrapowania wartości. Natomiast w wieloetapowym podejściu Rainbow agent uczy się na podstawie n-wielu kroków naprzód, co pozwala na lepsze uchwycenie długoterminowych zależności w grach, gdzie nagrody mogą być odłożone w czasie, jak np. w Super Mario Bros.

\subsection{Uczenie dystrybucyjne}
Uczenie dystrybucyjne (ang. Distributional Q-learning), wprowadzone przez Bellemare'a i in., to kolejne ważne rozszerzenie zaimplementowane w Rainbow. Tradycyjnie, algorytmy RL uczą się średnich wartości nagród, natomiast podejście dystrybucyjne polega na uczeniu się całej dystrybucji możliwych nagród. Dzięki temu agent lepiej radzi sobie z niepewnością i różnorodnością wyników w złożonych środowiskach. To rozszerzenie przyczyniło się do znaczącej poprawy wyników w wielu grach Atari.

\subsection{Noisy Nets}
Ostatnim z kluczowych komponentów Rainbow są sieci z warstwami szumu (Noisy Nets). Standardowe metody eksploracji, takie jak $\varepsilon$-greedy, są często niewystarczające w bardzo złożonych grach, takich jak Montezuma’s Revenge, gdzie agent musi wykonywać wiele akcji zanim otrzyma pierwszą nagrodę. Noisy Nets wprowadzają losowość do samej architektury sieci neuronowej, co pozwala agentowi na bardziej zaawansowaną eksplorację przestrzeni stanów, szczególnie w początkowych etapach treningu.

\subsection{Podsumowanie wyników Rainbow}
Wyniki eksperymentalne przedstawione przez Hessela i in. \cite{RAI} pokazują, że połączenie tych sześciu rozszerzeń pozwoliło na uzyskanie state-of-the-art wyników w benchmarku gier Atari 2600. W porównaniu z samodzielnym DQN oraz jego rozszerzeniami, Rainbow osiągnął znacznie lepszą efektywność danych oraz wyższą jakość wyników końcowych. Autorzy przeprowadzili również badania ablation, które potwierdziły, że każde z rozszerzeń Rainbow przyczynia się do poprawy wyników, choć niektóre z nich (np. priorytetyzowana pamięć doświadczeń i uczenie wieloetapowe) miały większy wpływ na końcowy wynik niż inne.

\section{Evolution Strategies jako skalowalna alternatywa dla uczenia ze wzmocnieniem}

Uczenie ze wzmocnieniem (Reinforcement Learning, RL) tradycyjnie opiera się na formalizmie procesów decyzyjnych Markowa (MDP) i optymalizacji funkcji wartości, co pozwala na uczenie agentów do podejmowania decyzji w złożonych środowiskach. Jednakże, w ostatnich latach, badacze poszukiwali alternatyw, które mogłyby lepiej skalować się na większą liczbę jednostek obliczeniowych i rozwiązywać problemy w bardziej efektywny sposób. Jednym z takich podejść są strategie ewolucyjne (Evolution Strategies, ES), które w pracy Salimansa i in. \cite{EV} zostały przedstawione jako skalowalna alternatywa dla popularnych technik RL, takich jak Q-learning i Policy Gradients.
\subsection{Podstawy Evolution Strategies (ES)}
Strategie ewolucyjne to klasa heurystycznych algorytmów optymalizacyjnych inspirowanych naturalną ewolucją. W ramach tych algorytmów, populacja wektorów parametrów (genotypów) jest modyfikowana poprzez losowe perturbacje (mutacje), a następnie oceniana na podstawie funkcji celu (fitness). Najlepsze parametry są następnie łączone, tworząc populację dla kolejnej iteracji. Główną zaletą ES jest to, że nie wymagają one obliczania gradientów, co czyni je mniej podatnymi na problemy takie jak niestabilność gradientów w głębokich sieciach neuronowych.

Salimans i in. \cite{EV} proponują naturalne strategie ewolucyjne (Natural Evolution Strategies, NES), które maksymalizują wartość funkcji celu poprzez gradientowe kroki optymalizacyjne w przestrzeni parametrów, z wykorzystaniem estymatora funkcji celu (tzw. score function estimator) \cite{EV}. Podejście to jest szczególnie użyteczne w środowiskach, w których nie można bezpośrednio obliczyć gradientów funkcji wartości, jak w przypadku złożonych symulacji fizycznych czy gier wideo.

\subsection{Zalety skalowalności i paralelizacji}
Jedną z kluczowych zalet ES, na którą zwracają uwagę Salimans i in. \cite{EV}, jest możliwość skalowania algorytmu na dużą liczbę równoległych jednostek obliczeniowych. W tradycyjnych metodach RL, takich jak Q-learning, komunikacja między jednostkami obliczeniowymi wymaga przesyłania całych gradientów, co zwiększa zapotrzebowanie na pasmo komunikacyjne. Natomiast w ES, jedyne informacje, które muszą być przesyłane między jednostkami, to skalarne wartości funkcji celu, co znacznie zmniejsza potrzebne zasoby komunikacyjne. Dzięki temu możliwe jest równoległe uruchomienie tysięcy jednostek, co prowadzi do drastycznego skrócenia czasu treningu. Na przykład, Salimans i in. zademonstrowali, że za pomocą ES udało się rozwiązać zadanie chodzenia humanoidalnego w symulatorze MuJoCo w zaledwie 10 minut, wykorzystując ponad 1440 równoległych jednostek obliczeniowych \cite{EV}.

\subsection{Efektywność danych i eksploracja}
Choć tradycyjne algorytmy RL, takie jak Policy Gradients czy Trust Region Policy Optimization (TRPO), są bardzo efektywne pod względem liczby danych potrzebnych do treningu, ES oferuje inne korzyści. Autorzy wykazali, że choć ES wymagały 3 do 10 razy więcej danych niż A3C (Asynchronous Advantage Actor-Critic) w środowiskach Atari, to redukcja wymagań obliczeniowych była na tyle znacząca, że wyniki były osiągane znacznie szybciej niż w przypadku tradycyjnych metod. W eksperymentach na zadaniach MuJoCo, ES dorównywały wynikami TRPO, choć zużywały do 10 razy więcej danych \cite{EV}.

Jednym z najbardziej interesujących wniosków płynących z pracy jest lepsze zachowanie eksploracyjne ES w porównaniu z metodami gradientowymi, takimi jak TRPO. Na przykład, w zadaniu humanoidalnego chodzenia w MuJoCo, ES były w stanie wypracować szeroką gamę stylów chodzenia, w tym chodzenie na boki i chodzenie tyłem, czego nie udało się osiągnąć za pomocą TRPO. Tego typu nietypowe rozwiązania wskazują na inne, jakościowo różne podejście do eksploracji przestrzeni stanów przez ES.

\subsection{Tolerancja na długie horyzonty czasowe i opóźnione nagrody}
Jednym z największych wyzwań w tradycyjnych metodach RL jest radzenie sobie z długimi horyzontami czasowymi i opóźnionymi nagrodami. Algorytmy oparte na gradientach, takie jak Q-learning, muszą dyskontować przyszłe nagrody, co wprowadza dodatkowe złożoności i często prowadzi do problemów ze stabilnością. W przeciwieństwie do tego, ES są niewrażliwe na częstotliwość akcji i opóźnione nagrody, ponieważ operują na pełnych epizodach, a nie na pojedynczych krokach. Oznacza to, że algorytm nie musi stosować dyskontowania nagród ani przybliżeń funkcji wartości, co czyni go bardziej odpornym na problemy z bardzo długimi sekwencjami akcji \cite{EV}.

\subsection{Zastosowanie w grach Atari}
Salimans i in. przetestowali ES na 51 grach Atari, używając tej samej architektury sieci neuronowej, co w DQN (Deep Q-Network). Wyniki pokazały, że ES osiągnęły konkurencyjne wyniki w stosunku do algorytmu A3C, osiągając lepsze wyniki w 23 grach, choć w 28 innych A3C okazał się skuteczniejszy. Dzięki silnej paralelizacji, czas treningu dla każdej gry został zredukowany do zaledwie jednej godziny \cite{EV}.

\subsection{Podsumowanie wyników ES}
Praca Salimansa i in. \cite{EV} pokazuje, że Evolution Strategies mogą być efektywną i skalowalną alternatywą dla tradycyjnych algorytmów RL opartych na gradientach. Chociaż wymagają one większej ilości danych, to znacznie redukują zapotrzebowanie na moc obliczeniową, co czyni je atrakcyjnymi w środowiskach o dużej liczbie równoległych jednostek obliczeniowych. Praca ta otwiera nowe możliwości dla stosowania ES w trudnych problemach, takich jak gry wideo czy złożone symulacje fizyczne, gdzie klasyczne metody RL mogą być mniej skuteczne.

\section{Ogólne uczenie ze wzmocnieniem w grach NES}

Wraz z postępem w dziedzinie uczenia ze wzmocnieniem (Reinforcement Learning, RL), gry wideo stały się popularnym środowiskiem testowym dla agentów uczących się. Gry takie jak Atari czy NES (Nintendo Entertainment System) dostarczają bogatych, złożonych przestrzeni stanów, które wymagają wyrafinowanych algorytmów do efektywnego działania. LeBlanc i Lee \cite{NES} przedstawili w swojej pracy podejście oparte na głębokim uczeniu ze wzmocnieniem (Deep Reinforcement Learning, DRL) dla gier NES, w szczególności badając trudności związane z przetwarzaniem danych wizualnych oraz trenowaniem agentów w środowisku NES bez dostarczania wiedzy eksperckiej.

\subsection{Wyzwania środowisk NES}
Jednym z głównych wyzwań, na które zwrócono uwagę w pracy \cite{NES}, jest większa złożoność gier NES w porównaniu z popularnymi wcześniej środowiskami testowymi, takimi jak Atari. Gry Atari, choć stanowiły ważny punkt odniesienia w badaniach nad RL, są stosunkowo proste w porównaniu z grami NES, które charakteryzują się bardziej złożoną fizyką, większą liczbą poziomów, a także bardziej złożoną mechaniką rozgrywki. NES wprowadza dodatkowe wyzwania związane z dynamiką gry i reakcjami czasu rzeczywistego, które są trudniejsze do opanowania przez algorytmy RL.

Aby skutecznie uczyć agentów w grach NES, autorzy podjęli wyzwanie skalowania metod uczenia głębokiego, uwzględniając specyficzne potrzeby związane z przestrzenią stanów i przetwarzaniem danych wejściowych. Jednym z kluczowych elementów było odpowiednie skalowanie i filtrowanie klatek, które były dostarczane jako dane wejściowe do modelu. Podobnie jak w przypadku Atari, w grach NES agent otrzymuje sygnały w postaci klatek obrazu (zestaw pikseli) reprezentujących aktualny stan gry. Skalowanie klatek i ich filtracja stanowiły kluczowy krok w zapewnieniu, że model może skutecznie przetwarzać informacje wizualne.

\subsection{Skalowanie i filtracja klatek}
W pracy LeBlanca i Lee \cite{NES} przyjęto podejście podobne do tego stosowanego w DQN (Deep Q-Network), gdzie obrazy wejściowe były najpierw przekształcane na szaro i skalowane do ustalonej rozdzielczości, co miało na celu redukcję wymagań obliczeniowych oraz uproszczenie analizy danych wizualnych. W kontekście gier NES, obrazy były skalowane do rozmiaru 84x84 pikseli, co jest standardową praktyką w uczeniu agentów RL, ponieważ zmniejsza złożoność wejściową, pozwalając jednocześnie na zachowanie istotnych informacji o stanie gry.

Ważnym aspektem filtracji klatek było także stosowanie technik pozwalających na eliminację "szumów" \ związanych z szybkim odświeżaniem obrazu w grach. Agent nie musiał analizować każdej pojedynczej klatki gry, co zmniejszało ilość danych wejściowych i poprawiało efektywność przetwarzania. W ten sposób, agent przetwarzał klatki w postaci serii obrazów zsumowanych w określonych odstępach czasowych, co pozwalało na zachowanie ciągłości dynamiki gry, a jednocześnie redukowało zbędne informacje.

\subsection{Optymalizacja hiperparametrów}
W pracy \cite{NES} szczegółowo omówiono konieczność dostosowania hiperparametrów w algorytmach RL do specyfiki gier NES. Na przykład, nagrody były odpowiednio dostosowywane, aby agent mógł szybciej uczyć się poprawnych strategii w bardziej złożonych grach. Autorzy wykazali, że w porównaniu do gier Atari, gry NES wymagały starannego dostrajania funkcji nagród, hiperparametrów uczenia się oraz schematów eksploracji, aby agent mógł lepiej dostosować się do zmieniających się warunków gry i różnych poziomów trudności.

\subsection{Trening agentów bez wiedzy eksperckiej}
Jednym z istotnych celów pracy było przetestowanie możliwości trenowania agentów bez dostarczania wiedzy eksperckiej. Gry NES, takie jak Super Mario Bros, charakteryzują się złożoną mechaniką ruchu i interakcji, a agent musiał samodzielnie odkrywać skuteczne strategie działania. W ramach tego podejścia, agenci RL byli trenowani przy użyciu wyłącznie danych wejściowych z gry (pikseli oraz informacji o nagrodach), bez wstępnie zaprogramowanych heurystyk czy wytycznych. Wyniki pokazały, że agentom udało się osiągnąć pewien poziom sukcesu, jednak proces ten wymagał więcej czasu i precyzyjnego dostrajania niż w przypadku prostszych środowisk.

\subsection{Rekomendacje i dalsze badania}
LeBlanc i Lee \cite{NES} zauważyli, że mimo osiągniętych postępów, gry NES wciąż stanowią wyzwanie dla współczesnych algorytmów RL. Jednym z głównych wniosków z pracy była potrzeba dalszego badania skutecznych metod eksploracji i lepszego dostrajania hiperparametrów w środowiskach o wysokiej dynamice, takich jak NES. Autorzy podkreślają, że przyszłe badania powinny skupić się na opracowywaniu bardziej zaawansowanych technik przetwarzania danych wizualnych oraz bardziej efektywnych metod eksploracji, które pozwoliłyby agentom na radzenie sobie w coraz bardziej złożonych środowiskach, zarówno w świecie gier, jak i w rzeczywistych zastosowaniach.

\subsection{Podsumowanie wyników}
Praca \cite{NES} podkreśla wyzwania i potencjał wykorzystania RL w grach NES, wskazując, że odpowiednia filtracja i skalowanie klatek, optymalizacja hiperparametrów oraz trenowanie agentów bez wiedzy eksperckiej stanowią kluczowe elementy sukcesu w tych środowiskach. Gry NES, takie jak Super Mario Bros, oferują znacznie bardziej złożone wyzwania niż Atari, a sukces agentów RL w tym środowisku może otworzyć drzwi do zastosowań RL w bardziej zaawansowanych grach i rzeczywistych problemach.

\section{Double Q-learning w głębokim uczeniu ze wzmocnieniem}

Jednym z najważniejszych wyzwań związanych z algorytmami uczenia ze wzmocnieniem, a w szczególności z algorytmem Q-learning, jest problem nadmiernego optymizmu w szacowaniu wartości akcji. W standardowym Q-learningu wybór akcji oraz jej ocena odbywają się w oparciu o te same wartości, co prowadzi do zawyżonych oszacowań wartości nagród, zwłaszcza w dynamicznych środowiskach o dużej liczbie możliwych akcji. W pracy van Hasselta, Gueza i Silvera \cite{DQ} autorzy przedstawili algorytm Double Q-learning, który został zaprojektowany, aby rozwiązać ten problem poprzez oddzielenie procesu wyboru akcji od jej oceny, co pozwala na bardziej stabilne uczenie się i poprawę jakości polityki agenta.

\subsection{Problem nadmiernych oszacowań w Q-learningu}
Standardowy algorytm Q-learning opiera się na maksymalizacji wartości akcji. Wartości te są aktualizowane na podstawie obserwowanych nagród, ale maksymalizacja oszacowanych wartości prowadzi do problemu zwanego "nadmiernym optymizmem" (ang. overestimation bias). Problem ten pojawia się, gdy maksymalizacja wartości prowadzi do preferowania zawyżonych wartości, które wynikają z szumów lub błędów w oszacowaniach funkcji wartości.

Van Hasselt i in. \cite{DQ} przeprowadzili szczegółowe badania nad algorytmem Deep Q-Network (DQN), który łączy Q-learning z głębokimi sieciami neuronowymi. Wykazali, że DQN cierpi na problem nadmiernych oszacowań wartości w grach Atari 2600, co prowadziło do suboptymalnych polityk. Autorzy udowodnili, że nawet w deterministycznych środowiskach, takich jak Atari, gdzie sieć neuronowa dostarcza elastyczne przybliżenie funkcji wartości, problem nadmiernego optymizmu wciąż występuje i negatywnie wpływa na wyniki agenta.

\subsection{Double Q-learning}
Double Q-learning, pierwotnie zaproponowany przez van Hasselta w 2010 roku, został w tej pracy zaadaptowany do środowisk z dużą skalą przybliżenia funkcji (np. sieci neuronowe). Główna idea Double Q-learning polega na oddzieleniu procesu wyboru akcji od jej oceny, co zmniejsza tendencję do nadmiernych oszacowań. W standardowym Q-learningu maksymalizacja wartości akcji odbywa się na podstawie tej samej funkcji wartości, co prowadzi do zawyżenia wartości. W Double Q-learningu wprowadza się dwie niezależne funkcje wartości, z których jedna służy do wyboru akcji, a druga do jej oceny.

W kontekście głębokiego uczenia, Double Q-learning został zaimplementowany z wykorzystaniem dwóch sieci neuronowych: sieci online i sieci celowej (target network). Sieć online jest używana do wyboru akcji, natomiast sieć celowa do oceny wartości tej akcji. Dzięki temu, Double Q-learning znacząco zmniejsza nadmierne oszacowania, co prowadzi do bardziej stabilnych i lepszych wyników w porównaniu do klasycznego DQN.

\subsection{Zastosowanie Double Q-learning w grach Atari}
Autorzy pracy przetestowali Double DQN na zestawie gier Atari 2600, który od lat stanowił standardowy benchmark dla algorytmów uczenia ze wzmocnieniem. Eksperymenty wykazały, że Double DQN nie tylko zredukował nadmierne oszacowania, ale także znacząco poprawił jakość polityk, które agent wypracowywał podczas nauki. Na przykład w grach takich jak Asterix czy Wizard of Wor, Double DQN wykazał znaczną poprawę wyników w porównaniu do klasycznego DQN. Co ciekawe, w tych przypadkach wzrost wartości szacowanych przez DQN zbiegł się z obniżeniem uzyskiwanych wyników w grze, co potwierdziło hipotezę, że nadmierne oszacowania negatywnie wpływają na jakość polityk.

\subsection{Wpływ na stabilność i jakość polityk}
Double Q-learning nie tylko poprawił dokładność wartości szacowanych przez agenta, ale również zwiększył stabilność procesu uczenia. Wyniki pokazały, że w większości testowanych gier Atari, agent trenowany z Double DQN uzyskał wyższe wyniki niż ten trenowany przy użyciu standardowego DQN. Zwiększona dokładność wartości akcji przełożyła się na wyższe wyniki w wielu grach, w których tradycyjne podejście zawodziło z powodu problemów z nadmiernym optymizmem.

W pracy \cite{DQ} autorzy przeprowadzili także badania ablation, aby ocenić wpływ poszczególnych elementów algorytmu na wyniki końcowe. Wyniki pokazały, że wprowadzenie Double Q-learningu przynosi korzyści we wszystkich testowanych grach, choć skala tych korzyści różniła się w zależności od gry. Double DQN wykazał się szczególnie wysoką efektywnością w grach, w których maksymalizacja wartości miała kluczowe znaczenie dla wypracowania optymalnej strategii, takich jak Space Invaders i Zaxxon.

\subsection{Wnioski}
Praca van Hasselta i in. \cite{DQ} miała ogromne znaczenie dla dalszego rozwoju algorytmów RL, w szczególności w kontekście gier wideo. Double Q-learning znacząco poprawił stabilność i dokładność procesu uczenia, eliminując jeden z największych problemów tradycyjnego Q-learningu — nadmierne oszacowania. Dzięki temu algorytm ten stał się podstawą dla wielu późniejszych prac w dziedzinie uczenia ze wzmocnieniem, a jego zastosowanie w grach Atari pokazało, że ma on ogromny potencjał do zastosowania w bardziej złożonych środowiskach, takich jak Super Mario Bros i inne zaawansowane gry wideo.

\section{VIME: Maksymalizacja informacji w eksploracji oparta na wariacyjnym uczeniu}

Jednym z największych wyzwań w uczeniu ze wzmocnieniem (Reinforcement Learning, RL) jest efektywne balansowanie między eksploracją a eksploatacją. Eksploracja polega na badaniu nowych strategii, które mogą przynieść lepsze wyniki w przyszłości, podczas gdy eksploatacja skupia się na maksymalizacji aktualnych zysków w oparciu o dotychczasową wiedzę agenta. W przypadku dużych, złożonych przestrzeni stanów, tradycyjne techniki eksploracji, takie jak strategia $\epsilon$-greedy czy szum Gaussa, często okazują się nieefektywne. Praca Houthoofta i in. \cite{VIME} przedstawia innowacyjną metodę eksploracji opartą na maksymalizacji zysku informacji o dynamice środowiska, nazwaną Variational Information Maximizing Exploration (VIME).

\subsection{Motywacja do eksploracji oparta na ciekawości}
VIME wykorzystuje podejście oparte na ciekawości (curiosity-driven exploration), w którym agent podejmuje działania, które prowadzą do odkrywania nowych, niespodziewanych stanów środowiska. Zamiast stosować proste heurystyki eksploracyjne, takie jak dodawanie szumu do akcji, VIME wprowadza wewnętrzną motywację, by agent podejmował działania, które maksymalizują zysk informacji o dynamice środowiska. Koncepcja ciekawości polega na zachęcaniu agenta do eksploracji stanów, które są dla niego zaskakujące, czyli takich, które powodują znaczące zmiany w modelu dynamiki środowiska \cite{VIME}.

Główna idea stojąca za VIME opiera się na mierzeniu wzrostu informacji uzyskanej na temat modelu dynamiki środowiska w trakcie eksploracji. Agent utrzymuje dystrybucję swoich przekonań na temat dynamiki środowiska w formie modelu probabilistycznego. Każda interakcja z otoczeniem, która prowadzi do znaczących aktualizacji tego modelu, jest nagradzana wewnętrznie, co skłania agenta do eksplorowania nowych stanów.

\subsection{Maksymalizacja zysku informacji}
W praktyce, VIME dodaje składnik nagrody wewnętrznej do standardowej funkcji nagrody zewnętrznej. Nagroda wewnętrzna jest oparta na zysku informacji, czyli na miarze, o ile nowo zaobserwowany stan poprawia model dynamiki środowiska agenta. Aby efektywnie obliczyć ten zysk, autorzy pracy \cite{VIME} wykorzystali podejście wariacyjne, oparte na sieciach neuronowych Bayesa (BNN). W skrócie, BNN są rodzajem sieci neuronowych, które utrzymują probabilistyczną dystrybucję wag zamiast pojedynczych punktowych oszacowań. W VIME, sieć BNN przewiduje dynamikę środowiska, a następnie oblicza, jak nowe dane (nowo zaobserwowane stany) wpływają na aktualizację dystrybucji wag. Zysk informacji jest mierzony jako dywergencja Kullbacka-Leiblera (KL) między nową a starą dystrybucją wag \cite{VIME}.

Dzięki temu, VIME nie tylko optymalizuje politykę agenta w celu maksymalizacji nagrody zewnętrznej, ale jednocześnie prowadzi eksplorację, nagradzając agenta za odkrywanie zaskakujących stanów. To sprawia, że metoda ta działa lepiej niż standardowe heurystyki eksploracyjne, które często prowadzą do losowego błądzenia w przestrzeni stanów.

\subsection{Zastosowanie w zadaniach z rzadkimi nagrodami}
Jednym z największych wyzwań w RL są zadania z rzadkimi nagrodami, gdzie agent musi odkryć skuteczne strategie przy minimalnej ilości dostępnych sygnałów zwrotnych. Autorzy przetestowali VIME w takich środowiskach, jak MountainCar, CartPoleSwingup czy HalfCheetah, gdzie standardowe metody eksploracji często zawodzą. Wyniki pokazały, że VIME osiągnął znacznie lepsze wyniki w porównaniu do metod heurystycznych, umożliwiając agentowi skuteczniejsze eksplorowanie środowiska w poszukiwaniu nagród, nawet gdy te były początkowo bardzo rzadkie \cite{VIME}. W eksperymentach VIME okazał się szczególnie skuteczny w zadaniach, gdzie nagroda była przyznawana jedynie za spełnienie specyficznych warunków (np. wydostanie się z doliny w MountainCar).

\subsection{Implementacja i efektywność}
W pracy \cite{VIME} autorzy zastosowali BNN, w których dystrybucja wag jest modelowana jako w pełni zdegenerowana dystrybucja Gaussa. Dzięki temu obliczenia związane z aktualizacją wag i obliczaniem dywergencji KL były możliwe do wykonania w sposób efektywny obliczeniowo. Aby obliczyć nagrodę wewnętrzną, wykorzystywano historię interakcji agenta z otoczeniem i przechowywano te dane w buforze (ang. replay buffer). Dzięki zastosowaniu podejścia Bayesowskiego, VIME pozwala agentowi efektywnie uczyć się, jak przewidywać dynamikę środowiska i lepiej eksplorować nieznane przestrzenie stanów.

\subsection{Wnioski}
VIME wprowadza nowatorskie podejście do eksploracji w uczeniu ze wzmocnieniem, wykorzystując zysk informacji o dynamice środowiska jako motywację do eksploracji. Wyniki eksperymentalne pokazują, że metoda ta znacząco przewyższa tradycyjne heurystyki eksploracyjne, zwłaszcza w zadaniach z rzadkimi nagrodami. Dzięki użyciu wariacyjnych sieci neuronowych Bayesa, VIME jest skalowalną metodą, która może być z powodzeniem stosowana w złożonych, ciągłych przestrzeniach stanów i akcji.

\section{Deterministyczne algorytmy gradientu polityki}

Uczenie ze wzmocnieniem (Reinforcement Learning, RL) w środowiskach z ciągłymi przestrzeniami akcji stanowi wyzwanie, szczególnie w kontekście stosowania klasycznych metod opartych na gradientach stochastycznych. W pracy Silvera i in. \cite{GRAD} zaproponowano deterministyczne algorytmy gradientu polityki (Deterministic Policy Gradient Algorithms, DPG), które mają na celu przezwyciężenie niektórych ograniczeń algorytmów stochastycznych w środowiskach o dużej liczbie wymiarów akcji.

\subsection{Gradient polityki deterministycznej}
W tradycyjnych algorytmach gradientu polityki, polityka reprezentowana jest jako rozkład probabilistyczny, który stochastycznie wybiera akcje w zależności od stanu. Jednakże, obliczanie gradientu stochastycznej polityki może być problematyczne w wysokowymiarowych przestrzeniach akcji, gdyż wymaga integracji zarówno nad przestrzenią stanów, jak i akcji. Praca \cite{GRAD} proponuje alternatywne podejście w postaci deterministycznej polityki $\mu_\theta(s)$, która bezpośrednio określa akcję w danym stanie, co pozwala na uproszczenie obliczeń i znacznie zmniejszenie liczby próbek wymaganych do estymacji gradientu polityki.

Deterministyczny gradient polityki ma formę oczekiwania gradientu funkcji wartości akcji. W przeciwieństwie do gradientu stochastycznego, który integruje nad przestrzenią stanów i akcji, gradient deterministyczny integruje jedynie nad przestrzenią stanów, co znacząco zmniejsza liczbę potrzebnych próbek w środowiskach o dużych wymiarach przestrzeni akcji \cite{GRAD}.

\subsection{Eksploracja i eksploatacja w DPG}
Jednym z kluczowych wyzwań przy stosowaniu polityk deterministycznych jest zapewnienie odpowiedniej eksploracji przestrzeni stanów i akcji. W pracy \cite{GRAD} wprowadzono algorytm typu actor-critic działający poza polityką (off-policy), który wykorzystuje stochastyczną politykę behawioralną do eksploracji przestrzeni akcji, ale uczy się deterministycznej polityki docelowej (target policy), co pozwala na korzystanie z efektywności gradientu deterministycznego przy jednoczesnym zachowaniu odpowiedniej eksploracji.

Deterministyczne algorytmy gradientu polityki są szczególnie skuteczne w środowiskach o wysokowymiarowych przestrzeniach akcji, gdzie gradienty stochastyczne wymagają wielu prób, aby dokładnie oszacować gradient. Dzięki redukcji przestrzeni akcji, DPG pozwala na bardziej efektywne eksplorowanie dużych przestrzeni akcji, co jest szczególnie przydatne w zadaniach kontroli robotów, gdzie liczba parametrów akcji może być bardzo duża.

\subsection{Actor-Critic dla polityki deterministycznej}
Autorzy \cite{GRAD} przedstawili także szczegółowy opis algorytmu actor-critic dla polityki deterministycznej, który składa się z dwóch głównych komponentów: aktora, który aktualizuje parametry polityki deterministycznej, oraz krytyka, który estymuje funkcję wartości akcji. W klasycznym algorytmie actor-critic estymacja funkcji wartości akcji może prowadzić do błędów wynikających z aproksymacji. W celu uniknięcia tych problemów, autorzy wprowadzili tzw. zgodną aproksymację funkcji, która minimalizuje błąd w oszacowaniu funkcji wartości, co zapewnia, że aproksymacja nie zniekształca gradientu polityki \cite{GRAD}.

\subsection{Wyniki eksperymentalne}
W pracy \cite{GRAD} autorzy przeprowadzili eksperymenty na standardowych benchmarkach uczenia ze wzmocnieniem, w tym na problemie bandytów o dużej liczbie wymiarów akcji oraz na klasycznych zadaniach RL, takich jak MountainCar, Pendulum oraz Puddle World. Wyniki pokazały, że deterministyczne algorytmy gradientu polityki osiągnęły znacznie lepsze wyniki niż ich stochastyczne odpowiedniki, zwłaszcza w wysokowymiarowych przestrzeniach akcji.

Na przykład, w eksperymencie z problemem bandytów o 50 wymiarach akcji, algorytm DPG znacznie przewyższył klasyczne algorytmy gradientu stochastycznego pod względem efektywności prób i szybkości zbieżności. Podobne wyniki uzyskano w zadaniach MountainCar i Pendulum, gdzie DPG osiągnął lepsze wyniki niż algorytmy stochastyczne, przy mniejszej liczbie prób \cite{GRAD}.

\subsection{Wnioski}
Praca \cite{GRAD} wprowadziła deterministyczne algorytmy gradientu polityki, które oferują znaczące korzyści w środowiskach o dużej liczbie wymiarów akcji. Dzięki uproszczeniu procesu estymacji gradientu polityki oraz skutecznemu połączeniu eksploracji stochastycznej z efektywnością polityki deterministycznej, algorytmy te wykazały przewagę nad klasycznymi metodami w różnych zadaniach RL. DPG otwiera nowe możliwości w zastosowaniach takich jak sterowanie robotami i inne środowiska o dużych wymiarach przestrzeni akcji.

\section{Hands-On Machine Learning}
Na koniec, książka Gérona \cite{HML} dostarcza kompleksowego omówienia technik uczenia maszynowego, w tym uczenia ze wzmocnieniem, z naciskiem na narzędzia takie jak Scikit-Learn i TensorFlow. Książka ta stanowi doskonałe źródło wiedzy dla inżynierów i badaczy, którzy chcą wdrożyć nowoczesne techniki uczenia maszynowego, w tym RL, do rzeczywistych systemów, takich jak gry wideo.

